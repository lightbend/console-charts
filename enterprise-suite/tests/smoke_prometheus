#!/bin/bash -u

# A collection of prometheus-related smoke tests

source smokecommon

#
# launch test apps
#

set -e
for app in resources/app*.yaml; do
  kubectl apply -f "$app"
done
set +e

#
# prometheus fetchers
#

prom() {
  minikube service --namespace=lightbend --url expose-prometheus
}
PROM=$( busy_wait prom )

prom_query() {
  curl -fsG "$PROM/api/v1/query" --data-urlencode "query=$*"
}

prom_has_data() {
  results=$( prom_query "$@" | jq '.data.result | length' )
  ! [[ results -eq 0 ]]
  T $? timeseries: "$*"
}

prom_has_no_data() {
  results=$( prom_query "$@" | jq '.data.result | length' )
  [[ results -eq 0 ]]
  T $? timeseries: "$*"
}

#
# log fetchers
#

kube_state_metrics_logs() {
  kubectl logs --tail=100 --namespace=lightbend -l app=prometheus,component=kube-state-metrics
}
prom_logs() {
  kubectl logs --tail=100 --namespace=lightbend -l app=prometheus,component=server -c prometheus-server
}
node_exporter_logs() {
  kubectl logs --tail=100 --namespace=lightbend -l app=prometheus,component=node-exporter
}


# this might be useful? dunno
# prom_interval() {
#   prom_query "prometheus_target_interval_length_seconds" |
#    jq -r '.data.result[0].metric.interval' |
#    sed -e 's/[^0-9.]//g'
# }
# PROM_INTERVAL=$( busy_wait prom_interval )

# prom_three_scrapes  waits until we've seen three scrapes of given metric
#  two scrapes is needed to compute a rate, then three is needed for rate of rate

prom_three_scrapes() {
  results=$( prom_query "count_over_time($1[10m]) > 2" | jq '.data.result | length' )
  ! [[ results -eq 0 ]]
}

busy_wait prom_three_scrapes kube_pod_info
T $? Prometheus kube-state-metrics has at least three scrapes

busy_wait prom_three_scrapes node_cpu_percent
T $? Prometheus node-exporter has at least three scrapes



#
# prometheus
#

prom_scrapes() {
  prom_query "sum(up)" |
   jq -r '.data.result[0].value[1]'
}
PROM_SCRAPES=$( prom_scrapes )

[[ PROM_SCRAPES -ge 4 ]]
T $? "Prometheus scraping $PROM_SCRAPES targets"

PROM_LOGS=$( prom_logs )
! echo "$PROM_LOGS" | grep -qi warn
T $? 'Prometheus "warn" logs'

! echo "$PROM_LOGS" | grep -qi err
T $? 'Prometheus "err" logs'

# grep prometheus_ rules.json | jq -r '.parameters.metric'
prom_metrics=(
  prometheus_notifications_dropped_rate
  prometheus_notification_queue_percent
  prometheus_rule_evaluation_failures_rate
  prometheus_target_scrapes_exceeded_sample_limit_rate
  prometheus_tsdb_reloads_failures_rate
  prometheus_config_last_reload_successful
  prometheus_target_sync_percent
)

# grep prometheus_ rules.json | jq -r '.parameters.name'
prom_health=(
  prometheus_notifications_dropped
  prometheus_notification_queue
  prometheus_rule_evaluation_failures
  prometheus_target_too_many_metrics
  prometheus_tsdb_reloads_failures
  prometheus_target_down
  prometheus_config_reload_failed
  prometheus_scrape_time
)

for m in "${prom_metrics[@]}"; do
  prom_has_data "$m"
done

for m in "${prom_health[@]}"; do
  prom_has_data "model{name=\"$m\"}"
  prom_has_data "health{name=\"$m\"}"
done

# coherency tests
# data with "es_workload" should also have a "namespace" label
prom_has_no_data 'count({es_workload=~".+", namespace=""})'
# health should have a "es_workload" label, with a few known exceptions
prom_has_no_data 'health{es_workload="",name!~"node.*|kube_node.*|prometheus_target_down|scrape_time"}'
# kube_pod_info must have es_workload labels
prom_has_data 'kube_pod_info{es_workload=~".+"}'
prom_has_no_data 'kube_pod_info{es_workload=""}'

#
# kube-state-metrics
#

# grep kube_ rules.json | jq -r '.parameters.metric'
kube_state_metrics=(
  kube_pod_info
  kube_node_pressure
  kube_pod_ready
  kube_pod_container_restarts_rate
  kube_pod_failed
  kube_pod_not_running
  kube_workload_generation_lag
)

# grep kube_ rules.json | jq -r '.parameters.name'
kube_state_health=(
  kube_node_pressure
  restarts
  kube_pod_not_ready
  kube_pod_not_running
  kube_workload_generation_lag
)

for m in "${kube_state_metrics[@]}"; do
  prom_has_data "$m"
done

for m in "${kube_state_health[@]}"; do
  prom_has_data "model{name=\"$m\"}"
  prom_has_data "health{name=\"$m\"}"
done

KSM_LOGS=$( kube_state_metrics_logs )

! echo "$KSM_LOGS" | grep -qi "failed"
T $? 'kube-state-metrics "failed" logs'

! echo "$KSM_LOGS" | grep -qi warn
T $? 'kube-state-metrics "warn" logs'

! echo "$KSM_LOGS" | grep -qi err
T $? 'kube-state-metrics "err" logs'


#
# node-exporter
#

# grep node_ rules.json | jq -r '.parameters.metric'
node_metrics=(
  node_cpu_percent
  node_memory_usable_percent
  node_filesystem_free_percent
  node_network_error_rate
)

# grep node_ rules.json | jq -r '.parameters.name'
node_health=(
  node_high_cpu
  node_memory_pressure
  node_network_errors
)

for m in "${node_metrics[@]}" ; do
  prom_has_data "$m"
done

for m in "${node_health[@]}"; do
  prom_has_data "model{name=\"$m\"}"
  prom_has_data "health{name=\"$m\"}"
done

NODE_EXPORTER_LOGS=$( node_exporter_logs )

! echo "$NODE_EXPORTER_LOGS" | grep -qi "failed"
T $? 'node-exporter "failed" logs'

! echo "$NODE_EXPORTER_LOGS" | grep -qi warn
T $? 'node-exporter "warn" logs'

! echo "$NODE_EXPORTER_LOGS" | grep -qi err
T $? 'node-exporter "err" logs'

# app tests

# app via 'Pod' service discovery
app_instances='count( count by (instance) (ohai{es_workload="es-test", namespace="default"}) ) == 2'
app_data() {
  results=$( prom_query "$app_instances" | jq '.data.result | length' )
  ! [[ results -eq 0 ]]
}
busy_wait app_data

prom_has_data "$app_instances"

# app via 'Pod' service discovery with multiple metric ports
app_instances_with_multiple_ports='count( count by (instance) (ohai{es_workload="es-test-with-multiple-ports", namespace="default"}) ) == 4'
app_data_with_multiple_ports() {
  results=$( prom_query "$app_instances_with_multiple_ports" | jq '.data.result | length' )
  ! [[ results -eq 0 ]]
}
busy_wait app_data_with_multiple_ports

prom_has_data "$app_instances_with_multiple_ports"

# app via 'Service' service discovery
app_instances_via_service='count( count by (instance) (ohai{es_workload="es-test-via-service", namespace="default"}) ) == 2'
app_data_via_service() {
  results=$( prom_query "$app_instances_via_service" | jq '.data.result | length' )
  ! [[ results -eq 0 ]]
}
busy_wait app_data_via_service

prom_has_data "$app_instances_via_service"

# clean up apps

for app in resources/app*.yaml; do
  kubectl delete -f "$app"
done

test_summary
