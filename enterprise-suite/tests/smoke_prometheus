#!/bin/bash -u

# A collection of prometheus-related smoke tests

NAMESPACE=${NAMESPACE:=lightbend}

source smokecommon
source prometheus_common

#
# launch test apps
#

set -e
for app in resources/app*.yaml; do
  kubectl apply -f "$app" -n $NAMESPACE --wait
done

# Wait until all pods are up and running
# Two instances per deployment with the "test:prometheus-test" label
num_pods=$(( $(find resources -name 'app*.yaml' -exec grep 'test: prometheus-test' {} \; | wc -l ) * 2 ))
busy_wait pods_exist "test=prometheus-test" $num_pods
busy_wait pods_up

cleanup() {
  for app in resources/app*.yaml; do
    kubectl delete -f "$app" -n $NAMESPACE
  done
}

trap cleanup 0
set +e

busy_wait prom_three_scrapes kube_pod_info
T $? Prometheus kube-state-metrics has at least three scrapes

PROM_SCRAPES=$( prom_scrapes )
[[ PROM_SCRAPES -ge 4 ]]
T $? "Prometheus scraping $PROM_SCRAPES targets"

PROM_LOGS=$( prom_logs )
! echo "$PROM_LOGS" | grep -qi warn
T $? 'Prometheus "warn" logs'

# It's okay if the reload fails, due to prometheus-server sometimes taking a little while to start up.
! echo "$PROM_LOGS" | grep -qi err | grep -v -e 'reload: context deadline exceeded'
T $? 'Prometheus "err" logs'

# grep prometheus_ rules.json | jq -r '.parameters.metric'
prom_metrics=(
  prometheus_notifications_dropped_rate
  prometheus_notification_queue_percent
  prometheus_rule_evaluation_failures_rate
  prometheus_target_scrapes_exceeded_sample_limit_rate
  prometheus_tsdb_reloads_failures_rate
  prometheus_config_last_reload_successful
  prometheus_target_sync_percent
)

# grep prometheus_ rules.json | jq -r '.parameters.name'
prom_health=(
  prometheus_notifications_dropped
  prometheus_notification_queue
  prometheus_rule_evaluation_failures
  prometheus_target_too_many_metrics
  prometheus_tsdb_reloads_failures
  prometheus_target_down
  prometheus_config_reload_failed
  prometheus_scrape_time
)

for m in "${prom_metrics[@]}"; do
  prom_has_data "$m"
done

for m in "${prom_health[@]}"; do
  prom_has_data "model{name=\"$m\"}"
  prom_has_data "health{name=\"$m\"}"
done

# coherency tests
# data with "es_workload" should also have a "namespace" label
prom_has_no_data 'count({es_workload=~".+", namespace="", name!~"kube_node.*", __name__!~"kube_node.*"})'
# health should have a "es_workload" label, with a few known exceptions
prom_has_no_data 'health{es_workload="", name!~"kube_node.*|prometheus_target_down|scrape_time"}'
# kube_pod_info must have es_workload labels
prom_has_data 'kube_pod_info{es_workload=~".+"}'
prom_has_no_data 'kube_pod_info{es_workload=""}'
# kube data mapped pod to workload labels
prom_has_no_data '{__name__=~ "kube_.+", pod!="", es_workload=""}'
# all container data have a workload label
prom_has_no_data '{__name__=~"container_.+", es_workload=""}'
# all targets should be reachable
prom_has_data 'up{kubernetes_name != "es-test-service-with-only-endpoints"} == 1'
prom_has_no_data 'up{kubernetes_name != "es-test-service-with-only-endpoints"} == 0'

#
# kube-state-metrics
#

# grep kube_ rules.json | jq -r '.parameters.metric'
kube_state_metrics=(
  kube_pod_info
  kube_pod_ready
  kube_pod_container_restarts_rate
  kube_pod_failed
  kube_pod_not_running
  kube_workload_generation_lag
)

# grep kube_ rules.json | jq -r '.parameters.name'
kube_state_health=(
  kube_container_restarts
  kube_pod_not_ready
  kube_pod_not_running
  kube_workload_generation_lag
)

for m in "${kube_state_metrics[@]}"; do
  prom_has_data "$m"
done

for m in "${kube_state_health[@]}"; do
  prom_has_data "model{name=\"$m\"}"
  prom_has_data "health{name=\"$m\"}"
done

KSM_LOGS=$( kube_state_metrics_logs )

! echo "$KSM_LOGS" | grep -qi "failed"
T $? 'kube-state-metrics "failed" logs'

! echo "$KSM_LOGS" | grep -qi warn
T $? 'kube-state-metrics "warn" logs'

! echo "$KSM_LOGS" | grep -qi err
T $? 'kube-state-metrics "err" logs'

# app tests
app_instances='count( count by (instance) (ohai{es_workload="es-test", namespace="'$NAMESPACE'"}) ) == 2'
busy_wait app_data "$app_instances"

prom_has_data "$app_instances"

# make_monitor "type/name" "metric"
make_monitor() {
  curl -s -XPOST -H "Content-Type: application/json" -H 'Author-Name: Me' -H 'Author-Email: me@lightbend.com' -H 'Message: testing!' \
    "$ES_MONITOR_API/monitors/$1" \
    --data @- <<EOF
    {
      "monitorVersion": "1",
      "model": "threshold",
      "parameters": {
        "metric": "$2",
        "window": "5m",
        "confidence": "1",
        "severity": {
          "warning": {
            "comparator": "!=",
            "threshold": "1"
          }
        },
        "summary": "summ",
        "description": "desc"
      }
    }
EOF
}

# Pod discovery - automatic metrics should gain an es_monitor_type label when a custom monitor is created
make_monitor "es-test/my_custom_monitor" "up"

busy_wait model_exists my_custom_monitor

prom_has_data 'up{es_workload="es-test", es_monitor_type="es-test"}'

# app via 'Pod' service discovery with multiple metric ports
app_instances_with_multiple_ports='count( count by (instance) (ohai{es_workload="es-test-with-multiple-ports", namespace="'$NAMESPACE'"}) ) == 4'
app_data_with_multiple_ports() {
  results=$( prom_query "$app_instances_with_multiple_ports" | jq '.data.result | length' )
  ! [[ results -eq 0 ]]
}
busy_wait app_data_with_multiple_ports

prom_has_data "$app_instances_with_multiple_ports"

# app via 'Service' service discovery
app_instances_via_service='count( count by (instance) (ohai{es_workload="es-test-via-service", namespace="'$NAMESPACE'"}) ) == 2'
app_data_via_service() {
  results=$( prom_query "$app_instances_via_service" | jq '.data.result | length' )
  ! [[ results -eq 0 ]]
}
busy_wait app_data_via_service

prom_has_data "$app_instances_via_service"

# 'Service' discovery - automatic metrics should gain an es_monitor_type label when a custom monitor is created
make_monitor "es-test-via-service/my_custom_monitor_for_service" "up"

busy_wait model_exists my_custom_monitor_for_service

prom_has_data 'up{es_workload="es-test-via-service", es_monitor_type="es-test-via-service"}'

# 'Service' discovery with only endpoints
app_instances_via_service_with_only_endpoints='count( count by (instance) (up{job="kubernetes-services", kubernetes_name="es-test-service-with-only-endpoints", namespace="'$NAMESPACE'"}) ) == 1'
app_data_via_service_with_only_endpoints() {
  results=$( prom_query "$app_instances_via_service_with_only_endpoints" | jq '.data.result | length' )
  ! [[ results -eq 0 ]]
}
busy_wait app_data_via_service_with_only_endpoints

prom_has_data "$app_instances_via_service_with_only_endpoints"


# Succeeds if all data for the workload $1 has a matching es_monitor_type
# Note we're currently ignoring health metrics because 'bad' data can stick around for 15m given their time window.
prom_all_workload_data_has_monitor_type() {
  results=$( prom_query "{es_workload=\"$1\", es_monitor_type!=\"$1\", __name__!=\"health\"}" | jq '.data.result | length' )
  [[ results -eq 0 ]]
}

# kubernetes-cadvisor metrics should have an es_monitor_type label.
make_monitor "es-test/es-monitor-type-test" "container_cpu_load_average_10s"

busy_wait model_exists es-monitor-type-test
# Specific test for regression of es-backend/issues/430.
prom_has_data '{job="kubernetes-cadvisor",es_monitor_type="es-test"}'

# This assumes we've processed enough data at this point that any non-conforming jobs will be caught.
busy_wait prom_all_workload_data_has_monitor_type es-test
T $? "timeseries: all data for es-test workload has a matching es-monitor-type label"

test_summary
