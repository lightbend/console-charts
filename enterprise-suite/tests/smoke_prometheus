#!/bin/bash -u

# A collection of prometheus-related smoke tests

source smokecommon

#
# launch test apps
#

set -e
NAMESPACE=${NAMESPACE:=lightbend}
for app in resources/app*.yaml; do
  kubectl apply -f "$app" -n $NAMESPACE
done

cleanup() {
  for app in resources/app*.yaml; do
    kubectl delete -f "$app" -n $NAMESPACE
  done
}

trap cleanup 0
set +e

#
# prometheus fetchers
#

ES_CONSOLE=$( busy_wait nodeport )
PROM="$ES_CONSOLE/service/prometheus"
ES_MONITOR_API="$ES_CONSOLE/service/es-monitor-api"

prom_query() {
  curl -fsG "$PROM/api/v1/query" --data-urlencode "query=$*"
}

prom_has_data() {
  test_prom_has_data () {
    results=$( prom_query "$@" | jq '.data.result | length' )
    ! [[ results -eq 0 ]]
  }
  timeout test_prom_has_data "$@"
  T $? timeseries: "$*"
}

prom_has_no_data() {
  test_prom_has_data () {
    results=$( prom_query "$@" | jq '.data.result | length' )
    [[ results -eq 0 ]]
  }
  timeout test_prom_has_data "$@"
  T $? timeseries: "$*"
}

model_exists() {
  results=$( prom_query "model{name=\"$1\"}" | jq '.data.result | length' )
  ! [[ results -eq 0 ]]
}

#
# log fetchers
#

kube_state_metrics_logs() {
  kubectl logs --tail=100 --namespace=${NAMESPACE} -l app=prometheus,component=kube-state-metrics
}
prom_logs() {
  kubectl logs --tail=100 --namespace=${NAMESPACE} -l app=prometheus,component=server -c prometheus-server
}
node_exporter_logs() {
  kubectl logs --tail=100 --namespace=${NAMESPACE} -l app=prometheus,component=node-exporter
}


# this might be useful? dunno
# prom_interval() {
#   prom_query "prometheus_target_interval_length_seconds" |
#    jq -r '.data.result[0].metric.interval' |
#    sed -e 's/[^0-9.]//g'
# }
# PROM_INTERVAL=$( busy_wait prom_interval )

# prom_three_scrapes  waits until we've seen three scrapes of given metric
#  two scrapes is needed to compute a rate, then three is needed for rate of rate

prom_three_scrapes() {
  results=$( prom_query "count_over_time($1[10m]) > 2" | jq '.data.result | length' )
  ! [[ results -eq 0 ]]
}

busy_wait prom_three_scrapes kube_pod_info
T $? Prometheus kube-state-metrics has at least three scrapes

busy_wait prom_three_scrapes node_cpu
T $? Prometheus node-exporter has at least three scrapes



#
# prometheus
#

prom_scrapes() {
  prom_query "sum(up)" |
   jq -r '.data.result[0].value[1]'
}
PROM_SCRAPES=$( prom_scrapes )

[[ PROM_SCRAPES -ge 4 ]]
T $? "Prometheus scraping $PROM_SCRAPES targets"

PROM_LOGS=$( prom_logs )
! echo "$PROM_LOGS" | grep -qi warn
T $? 'Prometheus "warn" logs'

# It's okay if the reload fails, due to prometheus-server sometimes taking a little while to start up.
! echo "$PROM_LOGS" | grep -qi err | grep -v -e 'reload: context deadline exceeded'
T $? 'Prometheus "err" logs'

# grep prometheus_ rules.json | jq -r '.parameters.metric'
prom_metrics=(
  prometheus_notifications_dropped_rate
  prometheus_notification_queue_percent
  prometheus_rule_evaluation_failures_rate
  prometheus_target_scrapes_exceeded_sample_limit_rate
  prometheus_tsdb_reloads_failures_rate
  prometheus_config_last_reload_successful
  prometheus_target_sync_percent
)

# grep prometheus_ rules.json | jq -r '.parameters.name'
prom_health=(
  prometheus_notifications_dropped
  prometheus_notification_queue
  prometheus_rule_evaluation_failures
  prometheus_target_too_many_metrics
  prometheus_tsdb_reloads_failures
  prometheus_target_down
  prometheus_config_reload_failed
  prometheus_scrape_time
)

for m in "${prom_metrics[@]}"; do
  prom_has_data "$m"
done

for m in "${prom_health[@]}"; do
  prom_has_data "model{name=\"$m\"}"
  prom_has_data "health{name=\"$m\"}"
done

# coherency tests
# data with "es_workload" should also have a "namespace" label
prom_has_no_data 'count({es_workload=~".+", namespace="", name!~"node.*|kube_node.*", __name__!~"node.*|kube_node.*"})'
# health should have a "es_workload" label, with a few known exceptions
prom_has_no_data 'health{es_workload="", name!~"node.*|kube_node.*|prometheus_target_down|scrape_time"}'
# kube_pod_info must have es_workload labels
prom_has_data 'kube_pod_info{es_workload=~".+"}'
prom_has_no_data 'kube_pod_info{es_workload=""}'
# kube data mapped pod to workload labels
prom_has_no_data '{__name__=~ "kube_.+", pod!="", es_workload=""}'
# all container data have a workload label
prom_has_no_data '{__name__=~"container_.+", es_workload=""}'
# all targets should be reachable
prom_has_data 'up{kubernetes_name != "es-test-service-with-only-endpoints"} == 1'
prom_has_no_data 'up{kubernetes_name != "es-test-service-with-only-endpoints"} == 0'

#
# kube-state-metrics
#

# grep kube_ rules.json | jq -r '.parameters.metric'
kube_state_metrics=(
  kube_pod_info
  kube_pod_ready
  kube_pod_container_restarts_rate
  kube_pod_failed
  kube_pod_not_running
  kube_workload_generation_lag
)

# grep kube_ rules.json | jq -r '.parameters.name'
kube_state_health=(
  kube_container_restarts
  kube_pod_not_ready
  kube_pod_not_running
  kube_workload_generation_lag
)

for m in "${kube_state_metrics[@]}"; do
  prom_has_data "$m"
done

for m in "${kube_state_health[@]}"; do
  prom_has_data "model{name=\"$m\"}"
  prom_has_data "health{name=\"$m\"}"
done

KSM_LOGS=$( kube_state_metrics_logs )

! echo "$KSM_LOGS" | grep -qi "failed"
T $? 'kube-state-metrics "failed" logs'

! echo "$KSM_LOGS" | grep -qi warn
T $? 'kube-state-metrics "warn" logs'

! echo "$KSM_LOGS" | grep -qi err
T $? 'kube-state-metrics "err" logs'


#
# node-exporter
#

# grep node_ rules.json | jq -r '.parameters.metric'
node_metrics=(
  node_cpu
  node_memory_MemAvailable
  node_filesystem_free
  node_network_transmit_errs
)

for m in "${node_metrics[@]}" ; do
  prom_has_data "$m"
done

NODE_EXPORTER_LOGS=$( node_exporter_logs )

! echo "$NODE_EXPORTER_LOGS" | grep -qi "failed"
T $? 'node-exporter "failed" logs'

! echo "$NODE_EXPORTER_LOGS" | grep -qi warn
T $? 'node-exporter "warn" logs'

! echo "$NODE_EXPORTER_LOGS" | grep -qi err
T $? 'node-exporter "err" logs'

# app tests
# app via 'Pod' service discovery
app_instances='count( count by (instance) (ohai{es_workload="es-test", namespace="'$NAMESPACE'"}) ) == 2'
app_data() {
  results=$( prom_query "$app_instances" | jq '.data.result | length' )
  ! [[ results -eq 0 ]]
}
busy_wait app_data

prom_has_data "$app_instances"

# make_monitor "type/name" "metric"
make_monitor() {
  curl -s -XPOST -H "Content-Type: application/json" -H 'Author-Name: Me' -H 'Author-Email: me@lightbend.com' -H 'Message: testing!' \
    "$ES_MONITOR_API/monitors/$1" \
    --data @- <<EOF
    {
      "monitorVersion": "1",
      "model": "threshold",
      "parameters": {
        "metric": "$2",
        "window": "5m",
        "confidence": "1",
        "severity": {
          "warning": {
            "comparator": "!=",
            "threshold": "1"
          }
        },
        "summary": "summ",
        "description": "desc"
      }
    }
EOF
}

# Pod discovery - automatic metrics should gain an es_monitor_type label when a custom monitor is created
make_monitor "es-test/my_custom_monitor" "up"

busy_wait model_exists my_custom_monitor

prom_has_data 'up{es_workload="es-test", es_monitor_type="es-test"}'

# app via 'Pod' service discovery with multiple metric ports
app_instances_with_multiple_ports='count( count by (instance) (ohai{es_workload="es-test-with-multiple-ports", namespace="'$NAMESPACE'"}) ) == 4'
app_data_with_multiple_ports() {
  results=$( prom_query "$app_instances_with_multiple_ports" | jq '.data.result | length' )
  ! [[ results -eq 0 ]]
}
busy_wait app_data_with_multiple_ports

prom_has_data "$app_instances_with_multiple_ports"

# app via 'Service' service discovery
app_instances_via_service='count( count by (instance) (ohai{es_workload="es-test-via-service", namespace="'$NAMESPACE'"}) ) == 2'
app_data_via_service() {
  results=$( prom_query "$app_instances_via_service" | jq '.data.result | length' )
  ! [[ results -eq 0 ]]
}
busy_wait app_data_via_service

prom_has_data "$app_instances_via_service"

# 'Service' discovery - automatic metrics should gain an es_monitor_type label when a custom monitor is created
make_monitor "es-test-via-service/my_custom_monitor_for_service" "up"

busy_wait model_exists my_custom_monitor_for_service

prom_has_data 'up{es_workload="es-test-via-service", es_monitor_type="es-test-via-service"}'

# 'Service' discovery with only endpoints
app_instances_via_service_with_only_endpoints='count( count by (instance) (up{job="kubernetes-services", kubernetes_name="es-test-service-with-only-endpoints", namespace="'$NAMESPACE'"}) ) == 1'
app_data_via_service_with_only_endpoints() {
  results=$( prom_query "$app_instances_via_service_with_only_endpoints" | jq '.data.result | length' )
  ! [[ results -eq 0 ]]
}
busy_wait app_data_via_service_with_only_endpoints

prom_has_data "$app_instances_via_service_with_only_endpoints"


# Succeeds if all data for the workload $1 has a matching es_monitor_type
# Note we're currently ignoring health metrics because 'bad' data can stick around for 15m given their time window.
prom_all_workload_data_has_monitor_type() {
  results=$( prom_query "{es_workload=\"$1\", es_monitor_type!=\"$1\", __name__!=\"health\"}" | jq '.data.result | length' )
  [[ results -eq 0 ]]
}

# kubernetes-cadvisor metrics should have an es_monitor_type label.
make_monitor "es-test/es-monitor-type-test" "container_cpu_load_average_10s"

busy_wait model_exists es-monitor-type-test
# Specific test for regression of es-backend/issues/430.
prom_has_data '{job="kubernetes-cadvisor",es_monitor_type="es-test"}'

# This assumes we've processed enough data at this point that any non-conforming jobs will be caught.
busy_wait prom_all_workload_data_has_monitor_type es-test
T $? "timeseries: all data for es-test workload has a matching es-monitor-type label"

test_summary
