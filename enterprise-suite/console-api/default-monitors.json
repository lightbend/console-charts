{
  "monitors": {
    "_": {
      "kube_container_restarting": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "kube_pod_container_restarts_rate",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "alertSummary": "container restarting rapidly",
          "alertDescription": "container {{$labels.container}} in pod {{$labels.pod}} of {{$labels.es_workload}} restarting rapidly",
          "monitorDescription": "Detects if a container in a pod is continously restarting. Only triggers for containers that start running. Can be due to getting OOM killed by OS or due to a bug that causes a crash shortly after starting.",
          "metricUnits": "restarts/s"
        }
      },
      "kube_pod_not_ready": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "kube_pod_ready",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "critical": {
              "comparator": "<",
              "threshold": "1"
            }
          },
          "alertSummary": "pod not ready",
          "alertDescription": "pod {{$labels.pod}} on {{$labels.es_workload}} not ready",
          "monitorDescription": "Detects when a pod does not reach readiness state.",
          "metricUnits": "boolean"
        }
      },
      "scrape_time": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "scrape_duration_seconds",
          "period": "15m",
          "minval": "3",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "numsigma": "3"
            }
          },
          "alertSummary": "scrape time anomalous",
          "alertDescription": "{{$labels.instance}} has anomalous scrape_duration_seconds"
        }
      },
      "lightbend_monitor_api_errors": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "es_monitor_api_last_error",
          "window": "1m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "alertSummary": "lightbend console monitor errors",
          "alertDescription": "Lightbend console api on {{$labels.instance}} has monitor errors"
        }
      },

      "akka_inbox_growth": {
        "monitorVersion": "1",
        "model": "growth",
        "parameters": {
          "metric": "akka_actor_mailbox_size",
          "filters": {
            "quantile": "0.5"
          },
          "period": "15m",
          "minslope": "0.1",
          "confidence": "1",
          "severity": {
            "critical": {
              "window": "5m"
            }
          },
          "alertSummary": "actor inbox growing",
          "alertDescription": "actor {{$labels.actor}} in {{$labels.app}} on {{$labels.instance}} has a growing inbox"
        }
      },
      "akka_processing_time": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "akka_actor_processing_time_ns",
          "filters": {
            "quantile": "0.5"
          },
          "period": "15m",
          "minval": "100000000",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "alertSummary": "actor processing time is anomalous",
          "alertDescription": "actor {{$labels.actor}} in {{$labels.app}} on {{$labels.instance}} has unusual processing time"
        }
      },
      "prometheus_notifications_dropped": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_notifications_dropped_rate",
          "window": "10m",
          "confidence": "0.25",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "alertSummary": "Prometheus dropping notifications",
          "alertDescription": "Prometheus dropping alerts sent to Alertmanager"
        }
      },
      "prometheus_notification_queue": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_notification_queue_percent",
          "window": "10m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "50"
            }
          },
          "alertSummary": "Prometheus alert queue filling",
          "alertDescription": "Prometheus alert queue is staying over 50% full"
        }
      },
      "prometheus_rule_evaluation_failures": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_rule_evaluation_failures_rate",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "alertSummary": "Prometheus rule failures",
          "alertDescription": "Prometheus has {{$value}} rules failing"
        }
      },
      "prometheus_target_too_many_metrics": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_target_scrapes_exceeded_sample_limit_rate",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "alertSummary": "Prometheus target over limit",
          "alertDescription": "Prometheus target at {{$labels.instance}} has too many metrics"
        }
      },
      "prometheus_tsdb_reloads_failures": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_tsdb_reloads_failures_rate",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "critical": {
              "comparator": ">=",
              "threshold": "1"
            }
          },
          "alertSummary": "Prometheus tsdb reload failing",
          "alertDescription": "Prometheus had {{$value}} reload failures"
        }
      },
      "prometheus_target_down": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "up",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": "!=",
              "threshold": "1"
            }
          },
          "alertSummary": "metrics target down",
          "alertDescription": "cannot connect to {{$labels.instance}} metrics endpoint for {{$labels.job}} data"
        }
      },
      "prometheus_config_reload_failed": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_config_last_reload_successful",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "critical": {
              "comparator": "!=",
              "threshold": "1"
            }
          },
          "alertSummary": "prometheus bad config",
          "alertDescription": "current config for prometheus has errors, will prevent restarts"
        }
      },
      "prometheus_scrape_time": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_target_sync_percent",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "75"
            }
          },
          "alertSummary": "prometheus has long scrape times",
          "alertDescription": "prometheus is taking {{$value}}% of the {{$labels.interval}} interval to get {{$labels.scrape_job}} metrics from {{$labels.instance}}"
        }
      },
      "zookeeper_latency": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "zk_avg_latency",
          "period": "15m",
          "minval": "10",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "alertSummary": "Zookeeper Latency",
          "alertDescription": "ZooKeeper latency is not normal in {{$labels.es_workload}} on {{$labels.instance}}"
        }
      },
      "zookeeper_connections": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "zk_num_alive_connections",
          "period": "15m",
          "minval": "10",
          "window": "15m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "alertSummary": "Zookeeper live connections is not normal",
          "alertDescription": "Zookeeper live connection in {{$labels.es_workload}} is not normal on {{$labels.instance}}"
        }
      },
      "zookeeper_pending_syncs": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "zk_pending_syncs",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "critical": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "alertSummary": "Zookeeper pending-syncs is not normal",
          "alertDescription": "Zookeeper Pending syncs in {{$labels.es_workload}} is greater than 0."
        }
      },
      "zookeeper_open_file_descriptor": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "zk_open_file_descriptor_count",
          "period": "15m",
          "minval": "10",
          "window": "15m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "alertSummary": "Zookeeper open file descriptor growth",
          "alertDescription": "Zookeeper open file descriptors in {{$labels.es_workload}} is not normal in {{$labels.instance}}"
        }
      },
      "redis_keyspace_miss": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "redis_keyspace_miss_ratio",
          "period": "10m",
          "minval": "1",
          "window": "10m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "alertSummary": "Redis key space miss ratio growth",
          "alertDescription": "Observing shifts in Redis key space ratio on {{$labels.es_workload}} in {{$labels.instance}}"
        }
      },
      "redis_evictions": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "redis_evicted_keys_total",
          "period": "10m",
          "minval": "1",
          "window": "10m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "alertSummary": "Redis evictions growing",
          "alertDescription": "Redis evictions on {{$labels.es_workload}} are growing in {{$labels.instance}}"
        }
      },
      "redis_commands_processed": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "redis_commands_processed_total",
          "period": "5m",
          "minval": "10",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "3"
            }
          },
          "alertSummary": "Redis command processed",
          "alertDescription": "Redis commands processed on {{$labels.es_workload}} is not normal in {{$labels.instance}}"
        }
      },
      "redis_connections": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "redis_connected_clients",
          "period": "15m",
          "minval": "10",
          "window": "15m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "alertSummary": "Redis client connections is not normal",
          "alertDescription": "Redis client connections on {{$labels.es_workload}} is not normal in {{$labels.instance}}"
        }
      },
      "akka_http_server_response_time": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "akka_http_http_server_response_time_ns",
          "filters": {
            "quantile": "0.5"
          },
          "period": "15m",
          "minval": "100000000",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "alertSummary": "HTTP server response time is anomalous",
          "alertDescription": "{{$labels.app}} on {{$labels.instance}} has unusual response time"
        }
      },
      "akka_http_client_response_time": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "akka_http_http_client_http_client_service_response_time_ns",
          "filters": {
            "quantile": "0.5"
          },
          "period": "15m",
          "minval": "100000000",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "alertSummary": "HTTP client response time is anomalous",
          "alertDescription": "{{$labels.app}} on {{$labels.instance}} has unusual response time"
        }
      },
      "akka_http_server_5xx": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "akka_http_http_server_responses_5xx_rate",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "alertSummary": "HTTP 5xx errors",
          "alertDescription": "HTTP server at {{$labels.instance}} has 5xx errors"
        }
      },
      "play_http_client_response_time": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "play_http_client_play_client_service_response_time_ns",
          "filters": {
            "quantile": "0.5"
          },
          "period": "15m",
          "minval": "100000000",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "alertSummary": "HTTP client response time is anomalous",
          "alertDescription": "{{$labels.app}} on {{$labels.instance}} has unusual response time"
        }
      },
      "lagom_circuit_breaker_state": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "lagom_circuit_breaker_state",
          "window": "1m",
          "confidence": "5e-324",
          "severity": {
            "critical": {
              "comparator": "<",
              "threshold": "3"
            }
          },
          "alertSummary": "Circuit breaker tripped",
          "alertDescription": "Circuit breaker {{$labels.circuit_breaker}} tripped on {{$labels.instance}}"
        }
      },
      "kafka_consumer_throughput": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "kafka_consumer_topic_consumed_rate",
          "period": "15m",
          "minval": "1000",
          "window": "15m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "3"
            }
          },
          "alertSummary": "Kafka consumer throughput is anomalous",
          "alertDescription": "{{$labels.es_workload}} has unusual throughput on {{$labels.topic}}"
        }
      },
      "kafka_producer_throughput": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "kafka_producer_topic_send_rate",
          "period": "15m",
          "minval": "1000",
          "window": "15m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "3"
            }
          },
          "alertSummary": "Kafka producer throughput is anomalous",
          "alertDescription": "{{$labels.es_workload}} has unusual throughput on {{$labels.topic}}"
        }
      },
      "kafka_consumer_lag": {
        "monitorVersion": "1",
        "model": "growth",
        "parameters": {
          "metric": "kafka_consumer_topic_lag_max",
          "period": "15m",
          "minslope": "0.1",
          "confidence": "1",
          "severity": {
            "warning": {
              "window": "15m"
            }
          },
          "alertSummary": "consumergroup falling behind",
          "alertDescription": "{{$labels.es_workload}} has lag on {{$labels.topic}}"
        }
      }
    }
  }
}
