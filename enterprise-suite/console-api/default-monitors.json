{
  "monitors": {
    "_": {
      "kube_container_restarting": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "kube_pod_container_restarts_rate",
          "window": "15m",
          "confidence": "0.25",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "alertSummary": "container restarting rapidly",
          "alertDescription": "container {{$labels.container}} in pod {{$labels.pod}} of {{$labels.es_workload}} restarting rapidly",
          "monitorDescription": "Detects if a container in a pod is continuously restarting. Can be due to getting OOM killed by OS or due to a bug that causes a crash shortly after starting.",
          "metricUnits": "restarts/s",
          "docLink": "https://developer.lightbend.com/docs/console/current/monitors/kubernetes.html#kube-container-restarting"
        }
      },
      "kube_pod_not_ready": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "kube_pod_ready",
          "window": "1m",
          "confidence": "5e-324",
          "severity": {
            "warning": {
              "comparator": "<",
              "threshold": "1"
            }
          },
          "alertSummary": "pod not ready",
          "alertDescription": "pod {{$labels.pod}} on {{$labels.es_workload}} not ready",
          "monitorDescription": "Detects if the pod's readiness probe failed. Use Kubernetes to get more information on the pod's status.",
          "metricUnits": "boolean",
          "docLink": "https://developer.lightbend.com/docs/console/current/monitors/kubernetes.html#kube-pod-not-ready"
        }
      },
      "scrape_time": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "scrape_duration_seconds",
          "period": "15m",
          "minval": "3",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "numsigma": "3"
            }
          },
          "alertSummary": "scrape time anomalous",
          "alertDescription": "{{$labels.instance}} has anomalous scrape_duration_seconds"
        }
      },
      "lightbend_monitor_api_errors": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "es_monitor_api_last_error",
          "window": "1m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "alertSummary": "lightbend console monitor errors",
          "alertDescription": "Lightbend console api on {{$labels.instance}} has monitor errors"
        }
      },
      "akka_inbox_queue_time": {
        "monitorVersion": "1",
        "model": "growth",
        "parameters": {
          "metric": "akka_actor_mailbox_time_ms",
          "filters": {
            "quantile": "0.99"
          },
          "period": "5m",
          "minslope": "0.2",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "window": "1m"
            }
          },
          "alertSummary": "actor inbox time growing",
          "alertDescription": "actor {{$labels.actor}} in {{$labels.app}} on {{$labels.instance}} has a rapidly growing mailbox time",
          "monitorDescription": "Detects if amount of time messages spend in an inbox is growing.",
          "metricUnits": "ms",
          "docLink": "https://developer.lightbend.com/docs/console/current/monitors/akka.html#akka-inbox-queue-time"
        }
      },
      "akka_processing_time": {
        "monitorVersion": "1",
        "model": "growth",
        "parameters": {
          "metric": "akka_actor_processing_time_ms",
          "filters": {
            "quantile": "0.99"
          },
          "period": "5m",
          "minslope": "0.2",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "window": "1m"
            }
          },
          "alertSummary": "actor processing time is growing",
          "alertDescription": "actor {{$labels.actor}} in {{$labels.app}} on {{$labels.instance}} has growing processing time",
          "monitorDescription": "Detects if message processing time is growing.",
          "metricUnits": "ms",
          "docLink": "https://developer.lightbend.com/docs/console/current/monitors/akka.html#akka-processing-time"
        }
      },
      "prometheus_notifications_dropped": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_notifications_dropped_rate",
          "window": "10m",
          "confidence": "0.25",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "alertSummary": "Prometheus dropping notifications",
          "alertDescription": "Prometheus dropping alerts sent to Alertmanager"
        }
      },
      "prometheus_notification_queue": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_notification_queue_percent",
          "window": "10m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "50"
            }
          },
          "alertSummary": "Prometheus alert queue filling",
          "alertDescription": "Prometheus alert queue is staying over 50% full"
        }
      },
      "prometheus_rule_evaluation_failures": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_rule_evaluation_failures_rate",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "alertSummary": "Prometheus rule failures",
          "alertDescription": "Prometheus has {{$value}} rules failing"
        }
      },
      "prometheus_target_too_many_metrics": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_target_scrapes_exceeded_sample_limit_rate",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "alertSummary": "Prometheus target over limit",
          "alertDescription": "Prometheus target at {{$labels.instance}} has too many metrics"
        }
      },
      "prometheus_tsdb_reloads_failures": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_tsdb_reloads_failures_rate",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "critical": {
              "comparator": ">=",
              "threshold": "1"
            }
          },
          "alertSummary": "Prometheus tsdb reload failing",
          "alertDescription": "Prometheus had {{$value}} reload failures"
        }
      },
      "prometheus_target_down": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "up",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": "!=",
              "threshold": "1"
            }
          },
          "alertSummary": "metrics target down",
          "alertDescription": "cannot connect to {{$labels.instance}} metrics endpoint for {{$labels.job}} data"
        }
      },
      "prometheus_config_reload_failed": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_config_last_reload_successful",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "critical": {
              "comparator": "!=",
              "threshold": "1"
            }
          },
          "alertSummary": "prometheus bad config",
          "alertDescription": "current config for prometheus has errors, will prevent restarts"
        }
      },
      "prometheus_scrape_time": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_target_sync_percent",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "75"
            }
          },
          "alertSummary": "prometheus has long scrape times",
          "alertDescription": "prometheus is taking {{$value}}% of the {{$labels.interval}} interval to get {{$labels.scrape_job}} metrics from {{$labels.instance}}"
        }
      },
      "akka_http_server_response_time": {
        "monitorVersion": "1",
        "model": "growth",
        "parameters": {
          "metric": "akka_http_server_response_time_ms",
          "filters": {
            "quantile": "0.99"
          },
          "period": "5m",
          "minslope": "0.1",
          "confidence": "1",
          "severity": {
            "warning": {
              "window": "1m"
            }
          },
          "alertSummary": "HTTP server response time is anomalous",
          "alertDescription": "{{$labels.app}} on {{$labels.instance}} has unusual response time",
          "monitorDescription": "Detects if the HTTP server response times have been increasing.",
          "metricUnits": "ms",
          "docLink": "https://developer.lightbend.com/docs/console/current/monitors/akka.html#akka-http-server-response-time"
        }
      },
      "akka_http_client_response_time": {
        "monitorVersion": "1",
        "model": "growth",
        "parameters": {
          "metric": "akka_http_client_service_response_time_ms",
          "filters": {
            "quantile": "0.99"
          },
          "period": "5m",
          "minslope": "0.1",
          "confidence": "1",
          "severity": {
            "warning": {
              "window": "1m"
            }
          },
          "alertSummary": "HTTP client response time is anomalous",
          "alertDescription": "{{$labels.app}} on {{$labels.instance}} has a growing response time",
          "monitorDescription": "Detects if the Akka HTTP client request times have been increasing.",
          "metricUnits": "ms",
          "docLink": "https://developer.lightbend.com/docs/console/current/monitors/akka.html#akka-http-client-response-time"
        }
      },
      "akka_http_server_5xx": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "akka_http_http_server_responses_5xx_rate",
          "window": "1m",
          "confidence": "5e-324",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "alertSummary": "HTTP 5xx errors",
          "alertDescription": "HTTP server at {{$labels.instance}} has 5xx errors",
          "monitorDescription": "Detects if Akka HTTP server is producing 5xx errors.",
          "metricUnits": "errors/s",
          "docLink": "https://developer.lightbend.com/docs/console/current/monitors/akka.html#akka-http-server-5xx"
        }
      },
      "play_http_client_response_time": {
        "monitorVersion": "1",
        "model": "growth",
        "parameters": {
          "metric": "play_http_client_service_response_time_ms",
          "filters": {
            "quantile": "0.99"
          },
          "period": "5m",
          "minslope": "0.1",
          "confidence": "1",
          "severity": {
            "warning": {
              "window": "1m"
            }
          },
          "alertSummary": "HTTP client response time is growing",
          "alertDescription": "{{$labels.app}} on {{$labels.instance}} has a growing response time",
          "monitorDescription": "Detects if the Play HTTP client request times have been increasing.",
          "metricUnits": "ms"
        }
      },
      "lagom_circuit_breaker_state": {
        "monitorVersion": "1",
        "model": "threshold",
        "parameters": {
          "metric": "lagom_circuit_breaker_state",
          "window": "1m",
          "confidence": "5e-324",
          "severity": {
            "warning": {
              "comparator": "==",
              "threshold": "2"
            },
            "critical": {
              "comparator": "==",
              "threshold": "1"
            }
          },
          "alertSummary": "Circuit breaker tripped",
          "alertDescription": "Circuit breaker {{$labels.circuit_breaker}} tripped on {{$labels.instance}}",
          "monitorDescription": "Detects if a circuit breaker in a Lagom application has tripped."
        }
      },
      "pipelines_kafka_consumer_throughput": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "kafka_consumer_topic_consumed_rate",
          "period": "15m",
          "minval": "1000",
          "window": "15m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "3"
            }
          },
          "alertSummary": "Kafka consumer throughput is anomalous",
          "alertDescription": "{{$labels.es_workload}} has unusual throughput on {{$labels.topic}}"
        }
      },
      "pipelines_kafka_producer_throughput": {
        "monitorVersion": "1",
        "model": "sma",
        "parameters": {
          "metric": "kafka_producer_topic_send_rate",
          "period": "15m",
          "minval": "1000",
          "window": "15m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "3"
            }
          },
          "alertSummary": "Kafka producer throughput is anomalous",
          "alertDescription": "{{$labels.es_workload}} has unusual throughput on {{$labels.topic}}"
        }
      },
      "pipelines_kafka_consumer_lag": {
        "monitorVersion": "1",
        "model": "growth",
        "parameters": {
          "metric": "kafka_consumer_topic_lag",
          "period": "5m",
          "minslope": "0.1",
          "confidence": "1",
          "severity": {
            "warning": {
              "window": "1m"
            }
          },
          "alertSummary": "consumergroup falling behind",
          "alertDescription": "{{$labels.es_workload}} has lag on {{$labels.topic}}",
          "monitorDescription": "Detects if consumer topic lag is increasing.",
          "metricUnits": "records"
        }
      },

      "kube_container_restarts": null,
      "kube_pod_not_running": null,
      "akka_inbox_growth": null,
      "zookeeper_latency": null,
      "zookeeper_connections": null,
      "zookeeper_pending_syncs": null,
      "zookeeper_open_file_descriptor": null,
      "cassandra_write_latency": null,
      "cassandra_read_latency": null,
      "redis_keyspace_miss": null,
      "redis_evictions": null,
      "redis_commands_processed": null,
      "redis_connections": null,
      "kafka_under_replicated_partitions": null,
      "memcached_miss_ratio": null,
      "memcached_current_connections": null,
      "memcached_evictions": null,
      "kafka_consumer_throughput": null,
      "kafka_producer_throughput": null,
      "kafka_consumer_lag": null,

      "kafka_consumergroup_lag": null,
      "kafka_incoming_messages": null,
      "kafka_offline_partition": null,
      "kube_workload_generation_lag": null
    }
  }
}
